{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5b1b217",
   "metadata": {},
   "source": [
    "## < word2vec을 활용한 spam 메일 분류  >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ecc443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ham/spam 데이터 불러오기\n",
    "import pandas as pd\n",
    "\n",
    "ham_spam = pd.read_csv('ham_spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b172771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ham_spam 데이터에서 \"text\"와 \"category\"만 가져와서 리스트로 만들기\n",
    "data = list(zip(ham_spam['text'],ham_spam['category']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649bcbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text와 category(=label) 나누기\n",
    "text = [data[i][0] for i in range(len(data))]\n",
    "label = [data[i][1] for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ddbb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숫자 제거, 문자 아닌 것 제거하고 띄어쓰기(' ') 기준으로 split하여 단어 뽑아내기\n",
    "import re\n",
    "text1 = [re.sub('\\d+',' ',tmp) for tmp in text]\n",
    "text2 = [re.sub('\\W+',' ',tmp) for tmp in text1]\n",
    "text_split = [tmp.split(' ') for tmp in text2]\n",
    "text_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a85576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test 데이터 나누기\n",
    "from sklearn.cross_validation import train_test_split  # 버전에 따라 다름\n",
    "\n",
    "text_train, text_test, label_train, label_test = train_test_split(text_split, label, random_state = 0)\n",
    "# text(메일 내용)의 train과 test 를 나눠줌\n",
    "# 정답인 label(ham인지 spam인지)도 train, test 따로 나눠줌\n",
    "# random_state : 난수 고정 / test_size(default) : 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1875308",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text_train), len(text_test))   # 4169 / 1390\n",
    "print(len(label_train), len(label_test)) # 4169 / 1390"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f57d347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words 설정 후 제거하기\n",
    "# stop_words 설정하기(의미가 없다고 판단되는 단어들)\n",
    "stop_words = ['are','a','just','in','','am','Am','also','to',\n",
    "             'is','or','and','we','at','it','the','on','for','I','m',\n",
    "             'by','i','on','an','By','be','me','that','Up','But','Are']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b4c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words 제거 함수\n",
    "def rm_stop(token):\n",
    "    final = []\n",
    "    for words in token:\n",
    "        word_list = []\n",
    "        for word in words:\n",
    "            if word.split(\"/\")[0] not in stop_words:\n",
    "                word_list.append(word)\n",
    "        final.append(word_list)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525cbf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text에 stop_words 제거하기\n",
    "text_train = rm_stop(text_train)\n",
    "text_test = rm_stop(text_test)\n",
    "text_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaa03d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec(CBOW / skip-gram)\n",
    "# 단어의 문맥적 의미를 보존하면서 단어를 벡터로 표현하는 과정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b89e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Skip-gram (sg=1)\n",
    "model1 = Word2Vec(text_train, size=100, window=10, min_count=10, workers=4, sg=1)\n",
    "# CBOW (sg=0)\n",
    "model2 = Word2Vec(text_train, size=100, window=10, min_count=10, workers=4, sg=0)\n",
    "\n",
    "# size : 한 단어당 몇 차원의 벡터로 만들지\n",
    "# window : 앞뒤 몇개의 단어를 사용할지\n",
    "# min_count : 최소 등장 횟수(min_count이하인 단어는 제외)\n",
    "# sg : CBOW(=0)로 할지 skip-gram(=1)으로 할지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f07594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text embedding 하기\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acfffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 embedding\n",
    "class TfidfEmbeddingVectorizer:\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        \n",
    "    def transform(self, X):\n",
    "        tfidf = TfidfVectorizer(analyzer = lambda x : x) \n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_) \n",
    "        word2weight = defaultdict(lambda : max_idf, [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()]) \n",
    "        \n",
    "        array_list =[]\n",
    "        for words in X:\n",
    "            array_list.append(np.array(np.mean([self.word2vec[w]*word2weight[w] for w in words if w in self.word2vec] or [np.zeros(100)], axis = 0)))\n",
    "        return(array_list)\n",
    "\n",
    "vec_tf_skip_gram = TfidfEmbeddingVectorizer(w2v_skip_gram)\n",
    "vec_tf_CBOW = TfidfEmbeddingVectorizer(w2v_CBOW)\n",
    "\n",
    "# skip-gram\n",
    "train_tf_s = vec_tf_skip_gram.transform(text_train)\n",
    "test_tf_s = vec_tf_skip_gram.transform(text_test)\n",
    "# CBOW\n",
    "train_tf_c = vec_tf_CBOW.transform(text_train)\n",
    "test_tf_c = vec_tf_CBOW.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79360c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf1 = SVC(decision_function_shape='ovo') # SVM 만들기\n",
    "svc_clf_s = clf.fit(train_tf_s, label_train)  # skip-gram\n",
    "svc_clf_c = clf.fit(train_tf_c, label_train)  # CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7925fd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측값 뽑아내기\n",
    "svc_pred_s = svc_clf.predict(test_tf_s) # skip-gram\n",
    "svc_pred_c = svc_clf.predict(test_tf_c) # CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb78fd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 확인\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(label_test, svc_pred_s)) # skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b2f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(label_test, svc_pred_c)) # CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af37246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip_gram이 성능이 훨씬 좋다. \n",
    "# 따라서, skip_gram으로 처리한 text로 다른 모델(K-NN, RF)도 만들어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fba23b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbor\n",
    "from sklearn import neighbors, datasets\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier()\n",
    "knn_clf = clf.fit(train_tf_s, label_train)\n",
    "knn_pred = knn_clf.predict(test_tf_s)\n",
    "knn_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d887339",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(label_test, knn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967c8830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 포레스트\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "RF_clf = clf.fit(train_tf_s, label_train)\n",
    "RF_pred = RF_clf.predict(test_tf_s)\n",
    "RF_pred\n",
    "print(metrics.classification_report(label_test, RF_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
