{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f647a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandasas pd\n",
    "from konlpy.tag import Twitter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d050457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    # 데이터 로드 및 간단한 전처리\n",
    "    path = \"C:/Users/YY/Desktop/TB/Week05/NLP/\"\n",
    "    file = pd.read_csv(os.path.join(path, file_path), encoding='utf-8', index_col=0)\n",
    "    # 필요 없는 칼럼 삭제 및 이름 변경\n",
    "    file.drop(['from', 'Date'], axis=1, inplace=True)\n",
    "    file.rename(columns={'x':'contents'}, inplace=True)\n",
    "    print(\"loading done\")\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3231f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stopwords(file):\n",
    "    # stopwords 준비\n",
    "    lines = []\n",
    "    f = open(os.path.join(path, file), 'r')\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        lines.append(line)\n",
    "    f.close()\n",
    "    stopwords = set(re.sub('\\n', '', word) for word in lines)\n",
    "    print(list(stopwords)[0:10])\n",
    "    print(\"making stopwords done\")\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f84a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_id():\n",
    "    # 트위터 아이디를 제거해준다.\n",
    "    pattern = re.compile('.@+[A-Za-z0-9\\_]*:*')\n",
    "    tweets = [re.sub(pattern, ' ', sentence) for sentence in list(file['contents'])]\n",
    "    print(\"removing id done\")\n",
    "    return tweets\n",
    "class TweetTokenizer:\n",
    "    # 트윗을 토큰화함.\n",
    "    def __init__(self):\n",
    "        self.twitter = Twitter()\n",
    "        self.stopwords = stopwords\n",
    "    def nominalize(tweets, start, end):\n",
    "        nouns = []\n",
    "        for tweet in tweets[start:end]:\n",
    "            nouns.append(' '.join([noun for noun in twitter.nouns(str(tweet)) if not noun in stopwords]))\n",
    "            # print(len(nouns))\n",
    "        # document = ' '.join(nouns)\n",
    "        print(\"tokenizing done\")\n",
    "        return nouns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77f73d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_clustering():\n",
    "    # Bag of Words를 만듦\n",
    "    # 계산 비용 때문에 ngram을 (1, 1)로 max_features를 50으로 한정함\n",
    "    vect = CountVectorizer(min_df=0.001, encoding='utf-8', max_features=50, ngram_range=(1, 1))\n",
    "    bow = vect.fit_transform(nouns)\n",
    "    print(\"사전 길이: \", len(vect.vocabulary_))\n",
    "    # 학습을 위해 Dense Matrix로 바꿔줌\n",
    "    X = bow.toarray()\n",
    "    print(\"X shape: \", X.shape)\n",
    "    vect.get_feature_names()\n",
    "    dict = {'문재인':0, '남북정상회담':1, '지방선거':2, '자유한국당':3, '안철수':4, '더불어민주당':5,\n",
    "            '미투':6, '바른미래당':7, '보수':8, '서울시장':9, '진보':10, '박원순':11, '김문수':12}\n",
    "    # 실제 키워드: Y\n",
    "    Y = np.array(file['Keyword'].map(dict)).astype(int).reshape(-1, 1)\n",
    "    # 역시 계산 비용 때문에 kmeans외에 다른 클러스터링 방법을 적용하기 어려웠음\n",
    "    kmeans = KMeans(n_clusters=13)\n",
    "    kmeans.fit(X)\n",
    "    # 예측 결과: pred\n",
    "    pred = kmeans.predict(X).reshape(-1, 1)\n",
    "    # 예측 결과 + 실제 결과: result\n",
    "    result = np.concatenate([pred, Y], axis=1)\n",
    "    print(pd.Series(pred.reshape(-1, )).value_counts())\n",
    "    print(pd.Series(Y.reshape(-1, )).value_counts())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d3c4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    file = load_data('tweet.csv')\n",
    "    stopwords = make_stopwords('korean_stopwords.txt')\n",
    "    twitter = Twitter()\n",
    "    tweets = remove_id()\n",
    "    nouns = TweetTokenizer.nominalize(tweets, 0, 118570)\n",
    "    result = embedding_clustering()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabe5067",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Run\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaed811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 분석 및 한계점 파악\n",
    "# print를 통해 중간 결과 기록\n",
    "print(list(stopwords)[0:10])\n",
    "# ['이와같다면', '자', '그에 따르는', '영차', '얼마만큼', '양자', '막론하고', '아무도', '근거로', '이용하여']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0e9e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"사전 길이: \", len(vect.vocabulary_))\n",
    "# 사전 길이: 50\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c778c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "print(\"X shape: \", X.shape)\n",
    "# X shape: (118570, 50)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f2803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비지도학습을 통한 군집 분류\n",
    "print(pd.Series(pred.reshape(-1, )).value_counts())\n",
    "# 7 25403\n",
    "# 12 24670\n",
    "# 1 11498\n",
    "# 6 11219\n",
    "# 10 10595\n",
    "# 2 8117\n",
    "# 0 8080\n",
    "# 4 5044\n",
    "# 11 3448\n",
    "# 3 3084\n",
    "# 8 3009\n",
    "# 5 2612\n",
    "# 9 1791\n",
    "# dtype: int64\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e337626",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "# 실제 Keyword 분류\n",
    "print(pd.Series(Y.reshape(-1, )).value_counts())\n",
    "# 0 39300\n",
    "# 1 17885\n",
    "# 2 13530\n",
    "# 3 10447\n",
    "# 4 9834\n",
    "# 5 7228\n",
    "# 6 5391\n",
    "# 7 4375\n",
    "# 8 3602\n",
    "# 9 2962\n",
    "# 10 2381\n",
    "# 11 1311\n",
    "# 12 324\n",
    "# dtype: int64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
