{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e6c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as npimport numpy.linalg as lin\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from scipy import io\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfbae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = io.loadmat('mnist-original.mat') \n",
    "X = mnist['data'].T\n",
    "y = mnist['label'].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496c45f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data information\n",
    "\n",
    "# 7만개의 작은 숫자 이미지\n",
    "# 행 열이 반대로 되어있음 -> 전치\n",
    "# grayscale 28x28 pixel = 784 feature\n",
    "# 각 picel은 0~255의 값\n",
    "# label = 1~10 label이 총 10개인거에 주목하자\n",
    "# data를 각 픽셀에 이름붙여 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428dd850",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = [ 'pixel'+str(i) for i in range(X.shape[1]) ]\n",
    "df = pd.DataFrame(X,columns=feat_cols)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df에 라벨 y를 붙여서 데이터프레임 생성\n",
    "df['y'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6109d38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자 먼저 mnist data들을 train 8 test 2 의 비율로 분리를 하자\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1fb17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 잘 분리가 된 것을 확인할 수 있다\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea330b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 한번 원본 data, PCA data, LDA data를 구해보고 비교를 해보자\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(df[feat_cols].values)\n",
    "df['pca-one'] = pca_result[:,0]\n",
    "df['pca-two'] = pca_result[:,1] \n",
    "df['pca-three'] = pca_result[:,2]\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f1e170",
   "metadata": {},
   "outputs": [],
   "source": [
    "Explained variation per principal component: [0.09746116 0.07155445 0.06149531]\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('First and Second Principal Components colored by digit', fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd5e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    ax.scatter(df['pca-one'][df['y']==i]\n",
    "               , df['pca-two'][df['y']==i]\n",
    "               , s = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bf5bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.legend(range(10))\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be5e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가시적인 효과는 위의 scatter plot으로 확인 할 수 있으나 적당히 몇차원에서 stop해야 하는지 모르므로 다음의 룰들을 따르기로 했다# 1. elbow point\n",
    "# 2. kaiser's rule\n",
    "# 3. 누적 설명률 80~90%\n",
    "# pca는 설명변수들의 분포에 따른 축소이므로 target변수인 y값을 지워주도록 한다\n",
    "train_1 = train.drop(['y'], axis=1)\n",
    "train_label = pd.DataFrame(train['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6470622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 먼저 2번의 규칙을 기준으로 eigenvalue의 값이 1이 되는 근처 지점을 찾아보자\n",
    "# 손글씨 데이터의 픽셀값들이므로 구석진 곳같이 언제나 0이 되는 지점을 제외한 픽셀들은 대부분 의미를 갖는 거 같다\n",
    "# 차원을 784에서 650정도로 줄여야 eigenvalue가 1을 기준으로 왔다갔다 하는 것을 찾았다.\n",
    "for i in range(645,650):\n",
    "    pca = PCA(n_components=i)\n",
    "    pca.fit(train_1)\n",
    "    print(pca.explained_variance_.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e7591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그럼 1번의 방법을 적용하기 위해 약 좌우로 25개의 범주에서 plot을 한번 해보자\n",
    "x_list = range(625,675)\n",
    "y_list = list()\n",
    "for i in x_list:\n",
    "    pca = PCA(n_components=i)\n",
    "    pca.fit(train_1)\n",
    "    y_list.append(pca.explained_variance_.min())\n",
    "plt.plot(x_list,y_list)\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('eigenvalue')\n",
    "Text(0, 0.5, 'eigenvalue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e5019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elbow인지는 잘 모르겠지만 그래도 약 647~8정도에서 급락이후 다시 완만한 굴곡으로 내려간다 생각되었다# 지금은 elbow로 안보이지만 만약 저부분만 확대한다면 elbow로 봐도 되지 않을까\n",
    "# 그래서 n_components를 648로 잡았다.\n",
    "# 3번째 룰을 적용해보려 하였는데 거의 누적설명률이 100프로라고 봐도 무방해 보인다\n",
    "pca = PCA(n_components=648)\n",
    "pca.fit(train_1)\n",
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b13114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = pca.transform(train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82f2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LDA를 해보자 ###\n",
    "# 개인적인 생각이지만 LDA란 label의 개수-1 차원(베르누이 분포라면 단 1차원밖에 안된다)으로 차원을 축소시켜주는데\n",
    "# 각 data들의 label을 반영하여 축소된 차원에서 data들의 label간 군집의 mean값의 거리가 최대가되고 label간 군집내의 편차는 최소가 되도록\n",
    "# 분류를 한다.\n",
    "# 여기서 pca하기전의 원본data를 사용한다면 데이터분포의 큰 영향을 주지 않는 축이 포함되고 그러면 라벨의 분포에 따른 해당 축의 영향력이 적어\n",
    "# 오히려 차원축소후 투영시 bias 혹은 error로 작용할 것 같다\n",
    "# 그래서 pca로 줄인 차원에서 lda로 축소를 하기로 생각하였다\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "lda_result = lda.fit_transform(df[feat_cols].values, df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc33fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,10))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('LDA 1', fontsize = 15)\n",
    "ax.set_ylabel('LDA 2', fontsize = 15)\n",
    "ax.set_title('First and Second LDA colored by digit', fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5677205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    ax.scatter(lda_result[df['y']==i,0]\n",
    "            ,lda_result[df['y']==i,1]\n",
    "            , s = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a768394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.legend(range(10))\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5b8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca이전 데이터를 가지고 lda를 적용하여 2차원으로 줄여 plot한 그래프이다# 이것으로는 잘 모르겠다\n",
    "# label개수 만큼의 축 즉 dimension을 잡아주어야 각 label의 구분 속성이 다 반영될 것이라 생각하여 max인 10으로 잡았다\n",
    "lda = LinearDiscriminantAnalysis(n_components=10)\n",
    "lda_result = lda.fit(train_1, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76041f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = column_or_1d(y, warn=True)\n",
    "train_after_lda = lda.transform(train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4672d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 648의 제곱근은 약 25이므로 깊이를 25정도로 잡아보자\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=3000, max_depth=25,random_state=1, criterion='gini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc5ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as pydatetime\n",
    "time_old = pydatetime.datetime.now().timestamp()\n",
    "RF.fit(train_after_lda, train_label)\n",
    "time_new = pydatetime.datetime.now().timestamp()\n",
    "print(time_new-time_old) # second로 출력 약 11분가량 걸린다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379b1a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = pd.DataFrame(test.drop(['y'],axis=1))\n",
    "test_1 = pca.transform(test_1)\n",
    "test_after_lda = lda.transform(test_1)\n",
    "test_label = pd.DataFrame(test['y'])\n",
    "RF.score(test_after_lda, test_label)\n",
    "# 2000 n_estimator accuracy 0.9128\n",
    "# 3000 n_estimator accuracy 0.913357"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacf29d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 이제 그럼 original data를 가지고 RF를 해보자\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF_origin = RandomForestClassifier(n_estimators=3000, max_depth=25,random_state=0, criterion='gini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b3816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as pydatetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "time_old = pydatetime.datetime.now().timestamp()\n",
    "RF_origin.fit(train.drop(['y'], axis=1), pd.DataFrame(train['y']))\n",
    "time_new = pydatetime.datetime.now().timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90693537",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_new-time_old) # origin은 약 4분이 걸린다 시간이 짧은 것은 n_estimator를 3000에서 2000, max_depth를 25에서 5로 줄여서 그렇다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee706ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_origin.score(test.drop(['y'], axis=1), pd.DataFrame(test['y'])) # 그래도 그냥 한거보다는 성능이 좋아서 다행이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387b0648",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2번째 방법 knn을 사용해보자\n",
    "# knn의 경우 distance를 바탕으로하는 instance_based model로 현재 각 feature 즉 독립변수의 value들이 0-255로 딱히 scaling을\n",
    "# 하지 않아도 규격이 정해져 distance로 구분하기가 어쩌면 좋은 data set이지 않을까 하는 생각을 하였다\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=10, algorithm='auto')\n",
    "time_old = pydatetime.datetime.now().timestamp()\n",
    "neigh.fit(train_after_lda, train_label)\n",
    "time_new = pydatetime.datetime.now().timestamp()\n",
    "print(time_new-time_old) # 0.05초 걸린다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d53df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = pd.DataFrame(test.drop(['y'],axis=1))\n",
    "test_1 = pca.transform(test_1)\n",
    "test_after_lda = lda.transform(test_1)\n",
    "test_label = pd.DataFrame(test['y'])\n",
    "neigh.score(test_after_lda, test_label) # accuracy 약 91.6퍼센트 나온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec89af83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그냥 원래 데이터로 한번 해보자\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=10, algorithm='auto')\n",
    "time_old = pydatetime.datetime.now().timestamp()\n",
    "neigh.fit(train.drop(['y'], axis=1), pd.DataFrame(train['y']))\n",
    "time_new = pydatetime.datetime.now().timestamp()\n",
    "print(time_new-time_old) # 20초 걸린다 꽤나 길다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe86eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh.score(test.drop(['y'], axis=1), pd.DataFrame(test['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a73c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그냥 knn한것은 약 25분~30분이 걸렸고 정확도가 97퍼센트나 나온것을 볼 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e731bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결론\n",
    "# pca lda는 너무 많은 독립변수들을 차원을 낮혀 차원의 저주 혹은 고차원으로 인한 기하급수적으로 늘어나는 필요 데이터 량을 줄여주는데 큰\n",
    "# 역할을 하는 방법이다\n",
    "# mnist data set에서는 이미지 픽셀마다 값이 0-255 정해진 value들이 독립변수로 선택이 된다\n",
    "# 이떄 random forest같은 경우는 bagging의 한 종류로써 boostrapping을 하게되는데 여기서 독립변수 종류를 선택할 때 label에 관계없이\n",
    "# 언제나 0인 data set들만 선택이 된다던가 이들로 인해서 각 base model 즉 decision tree에서 이들이 섞여들어가면 gini계수 판별로\n",
    "# 영향력 없는 data종류는 선택이 되지 않아 하지않아도 되는 process를 하게되므로 영향력이 있는 feature들만 뽑아 이를 특정 depth의 깊이로\n",
    "# 어느정도 깊은 model이 나오도록하여 boosting으로 variance가 높은 DT들을 학습하도록 하면 더 좋은 성능을 발휘한다고 생각한다\n",
    "# 반면 knn의 경우 distance를 기반으로 하는데 현재 mnist의 data set은 상당히 distance를 구하기 좋은 set으로 나와있다고 생각한다\n",
    "# 이를 pca와 lda를 거침으로 인해 각 feature의 scale까지 완벽한 data들이 차원축소가 일어나면서 새로운 차원에서 feature의 분포가 변화가\n",
    "# 생기고 이로인해 distance가 영향을 받아 더 안좋은 결과를 내놓았다고 생각한다\n",
    "# 이를 종합해보면 instance_based classifier보다 model_based classifier에 pca lda를 적용하는 것이 적합하다고 생각이 든다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
