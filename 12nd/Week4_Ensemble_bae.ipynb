{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcd724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys, warnings\n",
    "if not sys.warnoptions: warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e9376",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 30, 'max_rows', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7afcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa233242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 패키지 임포트\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7fd006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 임포트\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ed1952",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_data.pkl\",\"rb\") as tr:\n",
    "    train = pickle.load(tr)\n",
    "with open(\"test_data.pkl\",\"rb\") as te:\n",
    "    test = pickle.load(te)\n",
    "display(train)\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b82828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "X_train = train.drop(['OC'],axis=1)\n",
    "y_train = train[['OC']]\n",
    "X_test = test.drop(['OC'],axis=1)\n",
    "y_test = test[['OC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e11a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 모델에 대하여 튜닝을 시켜 최적의 파라미터로 설정 학습하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba8de34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb 모델 튜닝 (grid search)\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd24873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "estimator = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc6928",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.05], #so called `eta` value\n",
    "              'max_depth': [6],\n",
    "              'min_child_weight': [11],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.8],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [5], #number of trees, change it to 1000 for better results\n",
    "              'missing':[-999],\n",
    "              'seed': [1337]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f64d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(estimator, parameters, n_jobs=5, \n",
    "                   cv=5, \n",
    "                   scoring='roc_auc',\n",
    "                   verbose=2, refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d5de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "print('Best parameters found by grid search are:', clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34952f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과창\n",
    "Best parameters found by grid search are: {'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 6, 'min_child_weight': 11, 'missing': -999, 'n_estimators': 5, 'nthread': 4, 'objective': 'binary:logistic', 'seed': 1337, 'silent': 1, 'subsample': 0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fdbad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그리드 서치 결과로 모델 학습\n",
    "xgb = XGBClassifier(colsample_bytree= 0.7, learning_rate= 0.05, max_depth= 6, min_child_weight= 11, missing= -999, n_estimators= 5, nthread= 4, objective= 'binary:logistic', seed= 1337, silent= 1, subsample= 0.8)\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47310f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient boost 모델 팔라미터 튜닝\n",
    " \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "eclf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d9a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 튜닝\n",
    "params ={\n",
    "    \"n_estimators\" : [10, 20, 30, 50, 100, 200],\n",
    "    \"learning_rate\" : [i for i in np.linspace(0.1,1, 10)]}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5, n_jobs=-1)\n",
    "grid = grid.fit(X_train, y_train)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e85e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적 파라미터값으로 학습\n",
    "gbc=GradientBoostingClassifier(n_estimators=200, learning_rate=0.1)\n",
    "gbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050cc68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightgbm 파라미터 튜닝\n",
    "import lightgbm as lgb\n",
    " \n",
    "estimator = lgb.LGBMClassifier(num_leaves=2)\n",
    " \n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.05, 0.5, 1],\n",
    "    'n_estimators': [20, 40, 60, 80, 100, 120]\n",
    "}\n",
    " \n",
    "grid = GridSearchCV(estimator, param_grid, cv=5, scoring='roc_auc')\n",
    " \n",
    "grid.fit(X_train, y_train)\n",
    " \n",
    "print('Best parameters found by grid search are:', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecac4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters found by grid search are: {'learning_rate': 1, 'n_estimators': 40}\n",
    "lgb=lgb.LGBMClassifier(learning_rate= 1, n_estimators= 40)\n",
    "lgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d83f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfc\n",
    "params ={\n",
    "    \"n_estimators\" : [10, 20, 30, 50, 100],\n",
    "    \"max_features\" : [1,2,3,4,5,6,7, 10, 15, 20, 25]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b7ec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5, n_jobs=-1)\n",
    "grid = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4b841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'max_features': 2, 'n_estimators': 100}\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=10,\n",
    "                              max_features=4, \n",
    "                              n_jobs=-1, oob_score=True)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f99798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm 모델\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(gamma='auto', probability=True)\n",
    "svc.fit(X_train, y_train) \n",
    "# svm은 튜닝을 돌리기엔 시간이 상당히 걸리는 모델이고, svm을 주로 모델 앙상블을 할 계획이 아니기에 파라미터 튜닝은 생략함.\n",
    "# svm 모델이 일반적인 모델과 달리 데이터 상에 이상치와 같이 되게 떨어져있는(?) 데이터를 맞추는데 좋다하여, svm을 앙상블에 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23608bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 기본적으로 아래 보이는 조합과 같이 여러 조합으로 기본 바닐라 앙상블을 시켰음.\n",
    "eclf = VotingClassifier(estimators=[('xgb', xgb), ('svc', svc), ('lgb', lgb)], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c437dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = xgb, bgc, rgc\n",
    "# model2 = xgb, bgc, lgb\n",
    "# model3 = xgb, svc, lgb\n",
    "eclf.fit(X_train, y_train)\n",
    "y_prob=eclf.predict_proba(X_test)\n",
    "y_probe=y_prob[:,1]/0.8 # 결과값을 분류하는 임계값은 보통 0.8로 주었고, 경우에 따라 prod값을 보고 0.7과 0.8로 유연하게 주었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b46c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=[]\n",
    "\n",
    "for i in range(len(y_probe)):\n",
    "    if y_probe[i]>=1:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "        \n",
    "model3_08=pd.DataFrame({'inst_id':test['inst_id'],'OC':y_pred}) # 위의 조합을 여러 데이터 셋을 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07b5977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. gmean 기법 사용.\n",
    "votingC  = VotingClassifier(estimators=[('xgb', xgb), ('gbc', gbc), ('lgb', lgb)], voting='soft')\n",
    "pred_1 = votingC.fit(X_train, y_train).predict_proba(X_test)[:,1]\n",
    "votingC  = VotingClassifier(estimators=[('xgb', xgb), ('gbc', gbc), ('rfc', rfc)], voting='soft')\n",
    "pred_2 = votingC.fit(X_train, y_train).predict_proba(X_test)[:,1]\n",
    "votingC  = VotingClassifier(estimators=[('xgb', xgb), ('svc', gbc), ('lgb', lgb)], voting='soft')\n",
    "pred_3 = votingC.fit(X_train, y_train).predict_proba(X_test)[:,1]\n",
    "# 여러 조합으로 데이터를 학습시키고 pred값 도출\n",
    "# 한번의 앙상블 결과보다는 여러 데이터셋으로 모델을 학습시키는 방식처럼! 여러 앙상블을 또 앙상블 시키는 2중 앙상블 개념을 적용하면 더 좋은 결과가 나올 것으로 생각.\n",
    "# 위의 모델에 의한 결과값은 다 같은 데이터에 단순 모델만 여러 조합으로 학습시켰음\n",
    "# 따라서, 각 pred간 모델 결과간의 유사도가 높습니다. 그러므로 단순히 이 pred값을 앙상블 시키는 것보다 gmean 멱평균 앙상블 시키는 것이 더 좋음.\n",
    "# 결론적으로 여러 조합의 모델 앙상블을 gmean 시킨 값을 최종 pred로 삼았습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34b06c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = (pred_1*pred_2*pred_3)**(1/3)\n",
    "y_probe=pred/0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fc58e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=[]\n",
    "\n",
    "for i in range(len(y_probe)):\n",
    "    if y_probe[i]>=1:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "        \n",
    "model123_08=pd.DataFrame({'inst_id':test['inst_id'],'OC':y_pred})\n",
    "model123_08\n",
    "model123_08.to_csv('model123_08.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
